{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab9b9160",
   "metadata": {},
   "source": [
    "# 내가 직접 설계해보는 모델 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7f53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd467ca",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a6574",
   "metadata": {},
   "source": [
    "### 마스크 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(input, pad_token=1):\n",
    "   \"\"\"\n",
    "   seq_len x seq_len 사이즈의 padding mask를 만드는 함수\n",
    "   padding 토큰위치를 false, 그 외 토큰을 true로 생성\n",
    "   input = [batch_size, seq_len(token_ids)]  dtype=int\n",
    "   output = [batch_size, num_head(1), seq_len(1), seq_len]   dtype=bool\n",
    "   \n",
    "   input tensor에서 pad_token과 같은 위치는 0, 다른 위치는 1로 만듦\n",
    "   batch=3, seq_len=6\n",
    "   [[1,1,1,0,0,0]\n",
    "    [1,1,1,1,1,1]\n",
    "    [1,1,1,1,0,0]]\n",
    "   \"\"\"\n",
    "   mask = (input != pad_token)\n",
    "\n",
    "   # [batch, seq_len] -> (attention score 행렬)[batch, num_head(1), seq_len(1), masked_token]\n",
    "   return mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "def create_look_ahead_mask(seq_len):\n",
    "   \"\"\"\n",
    "   look ahead 마스크를 생성하여 리턴\n",
    "   토큰이 자기 뒤 토큰을 컨닝하는것을 방지하기 위해 자기 뒤에있는 토큰을 가리는 mask\n",
    "   input = seq_len   dtype=int\n",
    "   output = [batch_size(1), num_head(1), seq_len, seq_len]   dtype=bool\n",
    "   \"\"\"\n",
    "   # 1로 채워진 (seq_len, seq_len) 크기의 행렬을 만듦\n",
    "   # torch.tril() 함수는 행렬의 하삼각 부분만 남기고 나머지는 0으로 만듦\n",
    "   mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "\n",
    "   # [seq_len, seq_len] -> [batch(1), num_head(1), seq_len, masked_token]\n",
    "   return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "def create_mask(input, pad_token=1):\n",
    "   \"\"\"\n",
    "   [seq_len x seq_len]의 mask를 batch_size만큼 만들어 리턴\n",
    "   padding, look_ahead mask를 합침\n",
    "   input = [batch_size, seq_len(input_ids)]\n",
    "   output = [batch_size, seq_len, seq_len]\n",
    "   \"\"\"\n",
    "   pad_mask = create_padding_mask(input,pad_token)\n",
    "   look_ahead_mask = create_look_ahead_mask(input.shape[-1])\n",
    "   return pad_mask * look_ahead_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfce6f24",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22caa96",
   "metadata": {},
   "source": [
    "### Embedding Layer\n",
    "\n",
    "token_ids를 vector로 embedding하고 위치 vector를 더하는 layer\n",
    "\n",
    "input = [batch_size, seq_len(input_ids)]  dtype=int     \n",
    "output = [batch_size, seq_len, d_model]   dtype=float\n",
    "\n",
    "\n",
    "init 알고리즘\n",
    "1. Embedding 클래스를 생성하여 embedding 가중치 행렬 생성\n",
    "2. max_seq_len 길이의 position vector를 생성\n",
    "\n",
    "forward 알고리즘     \n",
    "1. [vocab_size, d_model]의 가중치 행렬에서 [input_id,:]를 look up하여 token_vectors 생성\n",
    "2. token_vectors의 크기를 키움\n",
    "3. token_vectors와 position_vectors를 더함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb0286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "   def __init__(self, vocab_size, d_model, max_seq_len, dropout_rate=0.1):\n",
    "      super(EmbeddingLayer, self).__init__()\n",
    "      self.d_model = d_model\n",
    "\n",
    "      \"토큰 임베딩\"\n",
    "      self.token_embedding = nn.Embedding(vocab_size, self.d_model)\n",
    "\n",
    "\n",
    "      \"위치 임베딩\"\n",
    "      pe = torch.zeros(max_seq_len, self.d_model)\n",
    "      # 위치 인덱스 생성 : [0,1,2,...,max_seq_len-1]\n",
    "      position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "      # 분모 계산\n",
    "      div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-math.log(10000.0) / self.d_model))\n",
    "\n",
    "      # 짝수 인덱스에는 sin 함수 적용\n",
    "      pe[:, 0::2] = torch.sin(position * div_term)\n",
    "      # 홀수 인덱스에는 cos 함수 적용\n",
    "      pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "      # pe 차원을 [max_seq_len, d_model] -> [1(batch), max_seq,len, d_model]로 변경\n",
    "      pe = pe.unsqueeze(0)\n",
    "\n",
    "      # register_buffer: 모델의 파라미터는 아니지만, state_dict에 저장되어야하는 텐서\n",
    "      self.register_buffer('pe',pe)\n",
    "\n",
    "      \n",
    "      \"dropout\"\n",
    "      self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "   def forward(self, x):\n",
    "      \"\"\"\n",
    "      순전파 함수\n",
    "      \"\"\"\n",
    "\n",
    "      \"토큰 임베딩\"\n",
    "      token_emb = self.token_embedding(x)\n",
    "\n",
    "      # 토큰 임베딩값을 키워서 위치 임베딩값이 상대적으로 매우 작은 값이 되게함\n",
    "      token_emb = token_emb * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "      \"위치 인코딩 추가\"\n",
    "      # 입력 시퀀스 길이에 맞게 위치 인코딩 텐서를 잘라서 사용\n",
    "      # self.pe 크기: [1, max_seq_len, d_model] -> pos_enc 크기: [1, seq_len, d_model]\n",
    "      seq_len = x.size(1)\n",
    "      pos_enc = self.pe[:,:seq_len,:]\n",
    "\n",
    "      # 토큰 임베딩에 위치 인코딩을 더함\n",
    "      final_emb = token_emb + pos_enc\n",
    "\n",
    "      \"drop out\"\n",
    "      return self.dropout(final_emb)\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8e9689",
   "metadata": {},
   "source": [
    "### Self Attention Layer\n",
    "\n",
    "토큰간에 관계 수치를 계산하여 수치만큼 값을 섞어주는 layer\n",
    "\n",
    "input = [batch_size, seq_len, d_model]    dtype=float              \n",
    "output = [batch_size, seq_len, d_model]   dtype=float\n",
    "\n",
    "\n",
    "init 알고리즘\n",
    "1. [d_model, d_model] 크기의 QKV 가중치 행렬 생성(nn.Linear())\n",
    "\n",
    "2. [d_model, d_model] 크기의 out 가중치 행렬 생성\n",
    "\n",
    "forward 알고리즘      \n",
    "1. QKV 벡터를 input 벡터와 행렬곱을 함     \n",
    "\n",
    "2. n_head 만큼 QKV 벡터를 나눔     \n",
    "   [batch_size, seq_len, d_model] -> [batch_size, n_head, seq_len, d_model]\n",
    "\n",
    "3. QK를 행렬곱(output)하여 attention 값을 계산함      \n",
    "\n",
    "4. 내적값(output)을 softmax 함수를 통과하여 모든 요소의 합이 1이 되게 만듦         \n",
    "\n",
    "5. output과 V 벡터를 행렬곱 하여 내적값 비율에 따른 V를 섞어줌          \n",
    "\n",
    "6. n_head 수만큼의 V 벡터를 하나의 행렬로 연결해줌    \n",
    "   [batch_size, n_head, seq_len, d_head] -> [batch_size, seq_len, d_model]\n",
    "\n",
    "6. out_linear의 가중치 행렬과 행렬곱을 하여 각 헤드에서 계산된 값들을 섞어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c9fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "   def __init__(self, d_model, num_head, dropout=0.1):\n",
    "      super().__init__()\n",
    "      # 혹시 d_model가 nhead로 나누어떨어지지 않는다면 오류 발생\n",
    "      assert d_model % num_head == 0, \"d_model must be divisible by nhead\"\n",
    "\n",
    "      # 기본적인 파라미터 설정\n",
    "      self.d_model = d_model  # 임베딩 차원\n",
    "      self.num_head = num_head    # 어텐션 헤드 수\n",
    "      self.head_dim = d_model // num_head # 어텐션 차원\n",
    "\n",
    "      # QKV 선형 변환 레이어\n",
    "      # input을 Q, K, V로 변환하는 역할\n",
    "      # 각각 (d_model, d_model) 크기의 가중치 행렬을 사용\n",
    "      self.q_linear = nn.Linear(self.d_model, self.d_model)\n",
    "      self.k_linear = nn.Linear(self.d_model, self.d_model)\n",
    "      self.v_linear = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "      # 출력 선형 변환 레이어\n",
    "      self.out_linear = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "      # 드롭아웃 레이어\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "   \n",
    "   def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "      \"\"\"\n",
    "      계산된 QKV를 받아, 어텐션 계산을 한 후 \n",
    "      최종 계산된 output과 attention weights를 반환\n",
    "      \"\"\"\n",
    "      # 어텐션 점수 계산 (Q, K 내적)\n",
    "      scores = torch.matmul(Q, K.transpose(-2,-1) / math.sqrt(self.head_dim))\n",
    "\n",
    "      # 마스크 적용\n",
    "      if mask is not None:\n",
    "         scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "      # softmax를 통해 어텐션 가중치 계산\n",
    "      attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "      # 특정 토큰 관계에 과도하게 의존하는것을 방지하기 위한 dropout\n",
    "      attention_weights = self.dropout(attention_weights)\n",
    "      \n",
    "      # 어텐션 가중치를 바탕으로 V를 섞음\n",
    "      output = torch.matmul(attention_weights, V)\n",
    "\n",
    "      return output, attention_weights\n",
    "\n",
    "   def forward(self, x, mask=None):\n",
    "      \"\"\"\n",
    "      순전파 함수\n",
    "      \"\"\"\n",
    "      batch_size = x.size(0)\n",
    "\n",
    "      # QKV Linear 통과 후\n",
    "      # QKV[batch_size, seq_len, d_model] -> QKV[batch_size, n_head, seq_len, d_head]로 변경\n",
    "      q = self.q_linear(x).view(batch_size, -1, self.num_head, self.head_dim).transpose(1,2)\n",
    "      k = self.k_linear(x).view(batch_size, -1, self.num_head, self.head_dim).transpose(1,2)\n",
    "      v = self.v_linear(x).view(batch_size, -1, self.num_head, self.head_dim).transpose(1,2)\n",
    "\n",
    "      # 각 헤드에서 어텐션 계산\n",
    "      x, attention_weights = self.scaled_dot_product_attention(q,k,v,mask)\n",
    "\n",
    "      # 각 헤드가 해석한 내용을 통합하는 linear\n",
    "      output = self.out_linear(x.transpose(1,2).contiguous().view(batch_size,-1,self.d_model))\n",
    "\n",
    "      return output, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8d6c3",
   "metadata": {},
   "source": [
    "### Feed Forward Network Layer\n",
    "\n",
    "비선형 함수를 추가하고, 임베딩 차원을 확장 축소하는 과정을 거쳐 각 토큰의 정보를 독립적으로 더 깊이 처리하여 표현력을 높임\n",
    "\n",
    "\n",
    "init 알고리즘          \n",
    "1. [d_model, d_ffn] 크기의 가중치 행렬 생성 (차원 확장 linear)        \n",
    "\n",
    "2. [d_ffn, d_model] 크기의 가중치 행렬 생성 (차원 축소 linear)           \n",
    "\n",
    "3. 활성화(비선형) 함수 정의          \n",
    "\n",
    "\n",
    "forward 알고리즘\n",
    "1. [batch_size, seq_len, d_model] 크기의 input을 linear 가중치와 행렬곱하여 [batch_size, seq_len, d_ffn] 크기로 차원 확장\n",
    "\n",
    "2. 활성화 함수를 통과하여 비선형적 변환\n",
    "\n",
    "3. linear 가중치와 행렬곱하여 [batch_size, seq_len, d_model]로 차원 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c72f6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "   def __init__(self, d_model, d_ffn, dropout=0.1, activate=None):\n",
    "      super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "      # 차원 확장 리니어\n",
    "      self.up_scale = nn.Linear(d_model, d_ffn)\n",
    "      # 차원 축소 리니어\n",
    "      self.down_scale = nn.Linear(d_ffn, d_model)\n",
    "\n",
    "      # 활성화 함수\n",
    "      if activate is None:\n",
    "         self.activate = nn.ReLU()\n",
    "      else:\n",
    "         self.activate = activate\n",
    "\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "   \n",
    "   def forward(self, x):\n",
    "      # 차원 up scale\n",
    "      x = self.up_scale(x)\n",
    "      # 활성함수 적용\n",
    "      x = self.activate(x)\n",
    "      # dropout\n",
    "      x = self.dropout(x)\n",
    "      # 차원 down scale\n",
    "      x = self.down_scale(x)\n",
    "      return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2031a55a",
   "metadata": {},
   "source": [
    "### Transformer Layer\n",
    "Attention Layer + Normalization + FFN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "473608fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "   def __init__(self, d_model, d_ffn, n_head, dropout_rate=0.1, activate_fn = None, pre_norm=True):\n",
    "      super().__init__()\n",
    "\n",
    "      self.d_model = d_model\n",
    "      self.d_ffn = d_ffn\n",
    "      self.n_head = n_head\n",
    "      self.d_head = d_model / n_head\n",
    "      self.dropout_rate = dropout_rate\n",
    "      self.activate_fn = activate_fn\n",
    "      self.pre_norm = pre_norm\n",
    "\n",
    "      self.attn_layer = MultiHeadSelfAttention(\n",
    "         d_model=self.d_model,\n",
    "         num_head=self.n_head,\n",
    "         dropout=self.dropout_rate\n",
    "      )\n",
    "\n",
    "      self.ffn_layer = FeedForwardNetwork(\n",
    "         d_model=self.d_model,\n",
    "         d_ffn=self.d_ffn,\n",
    "         dropout=self.dropout_rate,\n",
    "         activate=self.activate_fn\n",
    "      )\n",
    "\n",
    "      self.norm_layer = nn.LayerNorm(self.d_model)\n",
    "\n",
    "      self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "   def forward(self, x, mask=None):\n",
    "      # pre 정규화\n",
    "      if self.pre_norm:\n",
    "         output = self.norm_layer(x)\n",
    "      else:\n",
    "         output = x\n",
    "      \n",
    "      # attn layer 통과\n",
    "      attn_output, _ = self.attn_layer(output, mask)\n",
    "      # 잔차 연결\n",
    "      linked_output = self.dropout(attn_output) + x\n",
    "\n",
    "      # post 정규화\n",
    "      if not self.pre_norm:\n",
    "         output = self.norm_layer(linked_output)\n",
    "      else:\n",
    "         output = linked_output\n",
    "      \n",
    "      # ffn layer 통과\n",
    "      output = self.ffn_layer(output)\n",
    "\n",
    "      return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b2bf0",
   "metadata": {},
   "source": [
    "### LM Head Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12723b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMHead(nn.Module):\n",
    "   def __init__(self, vocab_size, d_model):\n",
    "      super(LMHead, self).__init__()\n",
    "      self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "   def forward(self, x):\n",
    "      token = self.head(x)\n",
    "      return token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f17208",
   "metadata": {},
   "source": [
    "# 모델 조립"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61529368",
   "metadata": {},
   "source": [
    "### custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "227b7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderOnlyModel(nn.Module):\n",
    "   def __init__(self,n_blocks , vocab_size, d_model, d_ffn, n_head, max_seq_len, dropout_rate=0.1, pre_norm=True, activate_fn=None, pad_token_id=1):\n",
    "      \"\"\"\n",
    "      Decoder Only Transformer Model 초기화\n",
    "\n",
    "      Args:\n",
    "         n_blocks (int): 쌓을 디코더 블록의 개수\n",
    "         vocab_size (int): 단어 사전 크기\n",
    "         d_model (int): 모델의 기본 차원\n",
    "         d_ffn (int): FFN의 차원\n",
    "         n_head (int): 어텐션 헤드의 개수\n",
    "         max_seq_len (int): 최대 input token 크기\n",
    "         dropout_rate (int): 드롭아웃 비율(default=0.1)\n",
    "         pre_nrom (bool): Pre-norm 또는 Post-norm 선택(default=True)\n",
    "         activate_fn (fn): FFN 내부 활성화 함수(default=GELU)\n",
    "         pad_id (int): padding 토큰 id(default=1)\n",
    "      \"\"\"\n",
    "      super(TransformerDecoderOnlyModel, self).__init__()\n",
    "\n",
    "      self.n_blocks = n_blocks\n",
    "      self.vocab_size = vocab_size\n",
    "      self.d_model = d_model\n",
    "      self.d_ffn = d_ffn\n",
    "      self.n_head = n_head\n",
    "      self.max_seq_len = max_seq_len\n",
    "      self.dropout_rate = dropout_rate\n",
    "      self.pre_norm = pre_norm\n",
    "      self.activate_fn = activate_fn\n",
    "      self.pad_token_id = pad_token_id\n",
    "\n",
    "      self.embed_layer = EmbeddingLayer(self.vocab_size, self.d_model, self.max_seq_len, self.dropout_rate)\n",
    "\n",
    "      self.decoder_blocks = nn.ModuleList(\n",
    "         [\n",
    "            TransformerDecoder(self.d_model, self.d_ffn, self.n_head, self.dropout_rate, self.activate_fn, self.pre_norm)\n",
    "            for _ in range(n_blocks)\n",
    "         ]\n",
    "      )\n",
    "\n",
    "      if self.pre_norm:\n",
    "         self.final_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "      self.lm_head = LMHead(self.vocab_size, self.d_model)\n",
    "\n",
    "      # token embedding 가중치와 LM Head 가중치를 공유\n",
    "      self.lm_head.head.weight = self.embed_layer.token_embedding.weight\n",
    "\n",
    "      # 파라미터 가중치 초기화\n",
    "      self.apply(self._init_weights)\n",
    "\n",
    "   def _init_weights(self, module):\n",
    "      \"\"\"\n",
    "      모델의 파라미터를 초기화하는 함수\n",
    "      \"\"\"\n",
    "      if isinstance(module, nn.Linear):\n",
    "         # 선형 레이어의 가중치를 Xavier Uniform 방식으로 초기화\n",
    "         nn.init.xavier_uniform_(module.weight)\n",
    "         if module.bias is not None:\n",
    "            # 편향(bias)은 0으로 초기화\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "      elif isinstance(module, nn.Embedding):\n",
    "         # 임베딩 레이어의 가중치를 정규분포로 초기화\n",
    "         nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "      elif isinstance(module, nn.LayerNorm):\n",
    "         # LayerNorm의 편향은 0, 가중치는 1로 초기화\n",
    "         nn.init.constant_(module.bias, 0)\n",
    "         nn.init.constant_(module.weight, 1.0)\n",
    "\n",
    "\n",
    "   \n",
    "   def forward(self, x):\n",
    "      \"\"\"\n",
    "      순전파 함수\n",
    "\n",
    "      Args:\n",
    "         x (torch.Tensor): 입력 토큰 ID 텐서 [batch_size, seq_len(token_ids)]\n",
    "      \"\"\"\n",
    "\n",
    "      # mask 생성\n",
    "      mask = create_mask(x, self.pad_token_id)\n",
    "\n",
    "      # 임베딩 layer 통과\n",
    "      output = self.embed_layer(x)\n",
    "\n",
    "      # Transformer block 통과\n",
    "      for decoder_block in self.decoder_blocks:\n",
    "         output = decoder_block(output, mask)\n",
    "\n",
    "      # 최종 정규화(Pre-Norm일 경우)\n",
    "      if self.pre_norm:\n",
    "         output = self.final_norm(output)\n",
    "      \n",
    "      # LM Head layer 통과\n",
    "      logits = self.lm_head(output)\n",
    "\n",
    "      return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ec7384",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f9d67e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 100])\n",
      "tensor([[[ 0.0606,  0.8559, -0.2472,  0.0537, -0.9774, -0.3505,  0.0290,\n",
      "          -0.3143,  0.8954,  0.1802,  0.0220,  0.0767,  0.3195, -0.2154,\n",
      "           0.4784, -0.5134,  0.2779,  0.3530, -0.2026,  0.1862,  0.1317,\n",
      "           0.0388,  0.4069, -0.4339,  0.4787,  0.5078, -0.0074,  0.1148,\n",
      "           0.2016,  0.3495,  0.2245,  0.4202, -0.8704, -0.0047, -0.4065,\n",
      "           0.5051, -0.2130,  0.1018,  0.6835, -0.3045,  0.2745, -0.3510,\n",
      "           0.9970, -0.4728, -0.1624,  0.1991,  0.9027,  0.7680, -0.6944,\n",
      "          -0.4747,  0.6820, -0.4628,  0.3635,  0.1804, -0.1201,  0.3186,\n",
      "           0.7449, -0.6342,  0.1991, -0.0769, -0.2550,  0.3060, -0.3434,\n",
      "          -0.2691, -0.4887,  0.7110, -0.4117,  0.2527, -0.0660, -0.9792,\n",
      "          -1.0190,  0.4599,  0.2004,  0.1995,  0.2572,  0.3660,  0.2949,\n",
      "           0.2403, -0.1207,  0.3486, -0.2585,  0.6885, -0.1083, -0.2716,\n",
      "          -0.8608, -0.3914, -0.4543,  0.0225,  0.3685, -0.1982, -0.0809,\n",
      "           0.3383,  0.4676,  0.2051, -0.3745,  0.2918,  0.5027,  0.1044,\n",
      "          -0.4060,  0.2163],\n",
      "         [ 0.0192,  0.8717, -0.2759,  0.0820, -0.9796, -0.3133, -0.0140,\n",
      "          -0.3454,  0.9823,  0.2122,  0.0506,  0.0602,  0.3380, -0.2512,\n",
      "           0.4771, -0.5922,  0.3268,  0.3618, -0.2199,  0.2044,  0.1137,\n",
      "           0.1068,  0.3964, -0.4275,  0.5220,  0.5264,  0.0220,  0.1338,\n",
      "           0.2229,  0.4000,  0.2533,  0.4324, -0.8715, -0.0062, -0.4608,\n",
      "           0.4890, -0.2237,  0.1521,  0.6834, -0.3015,  0.2675, -0.3281,\n",
      "           1.0211, -0.4944, -0.1412,  0.2150,  0.9025,  0.7902, -0.6389,\n",
      "          -0.4977,  0.7040, -0.4920,  0.3705,  0.1655, -0.1184,  0.2819,\n",
      "           0.6716, -0.6350,  0.1491,  0.0144, -0.3013,  0.2864, -0.3805,\n",
      "          -0.2858, -0.4114,  0.7322, -0.4249,  0.2712, -0.0038, -0.9904,\n",
      "          -1.0264,  0.4998,  0.1875,  0.1630,  0.2504,  0.3289,  0.2650,\n",
      "           0.2235, -0.0556,  0.3304, -0.2203,  0.6419, -0.0989, -0.2602,\n",
      "          -0.8675, -0.4123, -0.4299,  0.0197,  0.3989, -0.1575, -0.1127,\n",
      "           0.3279,  0.4462,  0.1790, -0.3474,  0.3558,  0.4926,  0.1037,\n",
      "          -0.4507,  0.2149],\n",
      "         [ 0.0016,  0.8779, -0.2906,  0.0913, -0.9793, -0.2922, -0.0321,\n",
      "          -0.3592,  1.0145,  0.2317,  0.0707,  0.0627,  0.3379, -0.2715,\n",
      "           0.4688, -0.6331,  0.3490,  0.3698, -0.2312,  0.2183,  0.1010,\n",
      "           0.1409,  0.3837, -0.4287,  0.5391,  0.5405,  0.0243,  0.1403,\n",
      "           0.2295,  0.4275,  0.2548,  0.4301, -0.8732, -0.0016, -0.4872,\n",
      "           0.4693, -0.2282,  0.1777,  0.6799, -0.2958,  0.2723, -0.3111,\n",
      "           1.0268, -0.5096, -0.1242,  0.2237,  0.9066,  0.7994, -0.6108,\n",
      "          -0.5050,  0.7115, -0.4940,  0.3731,  0.1567, -0.1151,  0.2634,\n",
      "           0.6338, -0.6351,  0.1273,  0.0598, -0.3191,  0.2825, -0.3989,\n",
      "          -0.3007, -0.3738,  0.7429, -0.4313,  0.2807,  0.0217, -0.9986,\n",
      "          -1.0334,  0.5224,  0.1821,  0.1481,  0.2390,  0.3080,  0.2543,\n",
      "           0.2132, -0.0253,  0.3237, -0.2014,  0.6170, -0.0883, -0.2492,\n",
      "          -0.8702, -0.4276, -0.4171,  0.0144,  0.4132, -0.1399, -0.1272,\n",
      "           0.3203,  0.4408,  0.1571, -0.3414,  0.3884,  0.4932,  0.1003,\n",
      "          -0.4698,  0.2157],\n",
      "         [-0.0107,  0.8804, -0.2871,  0.0912, -0.9721, -0.2678, -0.0492,\n",
      "          -0.3641,  1.0353,  0.2545,  0.0894,  0.0596,  0.3363, -0.2775,\n",
      "           0.4545, -0.6608,  0.3593,  0.3805, -0.2360,  0.2409,  0.0963,\n",
      "           0.1651,  0.3729, -0.4367,  0.5546,  0.5544,  0.0301,  0.1367,\n",
      "           0.2279,  0.4510,  0.2525,  0.4214, -0.8739,  0.0062, -0.4998,\n",
      "           0.4388, -0.2323,  0.1848,  0.6828, -0.2955,  0.2781, -0.3071,\n",
      "           1.0216, -0.5220, -0.1071,  0.2322,  0.9050,  0.8042, -0.6010,\n",
      "          -0.5174,  0.7226, -0.4826,  0.3808,  0.1498, -0.1154,  0.2443,\n",
      "           0.6114, -0.6352,  0.1286,  0.0999, -0.3186,  0.2747, -0.4115,\n",
      "          -0.3256, -0.3494,  0.7483, -0.4413,  0.2880,  0.0473, -1.0063,\n",
      "          -1.0334,  0.5571,  0.1774,  0.1368,  0.2234,  0.2914,  0.2411,\n",
      "           0.2171,  0.0068,  0.3218, -0.1991,  0.6116, -0.0712, -0.2372,\n",
      "          -0.8842, -0.4395, -0.4170,  0.0162,  0.4243, -0.1249, -0.1290,\n",
      "           0.3150,  0.4368,  0.1432, -0.3455,  0.4134,  0.4887,  0.0968,\n",
      "          -0.4710,  0.2146],\n",
      "         [-0.0299,  0.8841, -0.2677,  0.0907, -0.9563, -0.2247, -0.0836,\n",
      "          -0.3675,  1.0671,  0.3007,  0.1193,  0.0493,  0.3427, -0.2673,\n",
      "           0.4265, -0.7006,  0.3623,  0.4006, -0.2328,  0.2859,  0.0943,\n",
      "           0.1963,  0.3516, -0.4514,  0.5793,  0.5721,  0.0489,  0.1142,\n",
      "           0.2157,  0.4902,  0.2494,  0.4062, -0.8792,  0.0149, -0.5055,\n",
      "           0.3760, -0.2380,  0.1841,  0.6842, -0.3009,  0.2767, -0.3058,\n",
      "           1.0055, -0.5414, -0.0829,  0.2549,  0.8956,  0.8105, -0.5951,\n",
      "          -0.5420,  0.7419, -0.4591,  0.4000,  0.1494, -0.1079,  0.2130,\n",
      "           0.5846, -0.6281,  0.1435,  0.1673, -0.3079,  0.2569, -0.4296,\n",
      "          -0.3688, -0.3172,  0.7551, -0.4627,  0.3063,  0.0921, -1.0196,\n",
      "          -1.0223,  0.6178,  0.1648,  0.1156,  0.2000,  0.2647,  0.2149,\n",
      "           0.2375,  0.0651,  0.3293, -0.2023,  0.6170, -0.0405, -0.2161,\n",
      "          -0.9169, -0.4555, -0.4248,  0.0193,  0.4387, -0.1044, -0.1112,\n",
      "           0.2995,  0.4272,  0.1209, -0.3542,  0.4482,  0.4755,  0.0896,\n",
      "          -0.4732,  0.2069]]])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 100\n",
    "d_model = 10\n",
    "d_ffn = d_model * 4\n",
    "n_head = 2\n",
    "seq_len = 5\n",
    "batch_size = 1\n",
    "\n",
    "model = TransformerDecoderOnlyModel(6, vocab_size,d_model,d_ffn,n_head,seq_len)\n",
    "model.eval()\n",
    "\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "with torch.no_grad():\n",
    "   final_logits = model(input_ids)\n",
    "\n",
    "print(final_logits.shape)\n",
    "print(final_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1603ee7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
