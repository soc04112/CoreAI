{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 193300,
     "status": "ok",
     "timestamp": 1764035451970,
     "user": {
      "displayName": "Block All (BlockAll)",
      "userId": "16973279777064011498"
     },
     "user_tz": -540
    },
    "id": "4Ix79fC__MX0",
    "outputId": "903b4aac-44c0-47cb-f8bc-bde545bfa971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.8.0\n",
      "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.8.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (9.10.2.21)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.8.0)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.8.0)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.8.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (0.7.1)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch==2.8.0)\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.8.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.8.0)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch==2.8.0)\n",
      "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0) (3.0.3)\n",
      "Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m139.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.5.0\n",
      "    Uninstalling triton-3.5.0:\n",
      "      Successfully uninstalled triton-3.5.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
      "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.9.0+cu126\n",
      "    Uninstalling torch-2.9.0+cu126:\n",
      "      Successfully uninstalled torch-2.9.0+cu126\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.8.0 which is incompatible.\n",
      "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.8.0 triton-3.4.0\n",
      "Collecting torchaudio==2.8.0\n",
      "  Downloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchaudio==2.8.0) (2.8.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchaudio==2.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchaudio==2.8.0) (3.0.3)\n",
      "Downloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchaudio\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.9.0+cu126\n",
      "    Uninstalling torchaudio-2.9.0+cu126:\n",
      "      Successfully uninstalled torchaudio-2.9.0+cu126\n",
      "Successfully installed torchaudio-2.8.0\n",
      "Collecting torchvision==0.23.0\n",
      "  Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (2.0.2)\n",
      "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (2.8.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (11.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision==0.23.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision==0.23.0) (3.0.3)\n",
      "Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m136.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.24.0+cu126\n",
      "    Uninstalling torchvision-0.24.0+cu126:\n",
      "      Successfully uninstalled torchvision-0.24.0+cu126\n",
      "Successfully installed torchvision-0.23.0\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl size=256040057 sha256=f25da18657a87fc83dc1bfb8b7751b82246e9db355510226b674fd437c34b5fb\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/59/46/f282c12c73dd4bb3c2e3fe199f1a0d0f8cec06df0cccfeee27\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: flash-attn\n",
      "Successfully installed flash-attn-2.8.3\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
      "Downloading transformers-4.57.2-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m153.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.1\n",
      "    Uninstalling transformers-4.57.1:\n",
      "      Successfully uninstalled transformers-4.57.1\n",
      "Successfully installed transformers-4.57.2\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.8.0\n",
    "!pip install torchaudio==2.8.0\n",
    "!pip install torchvision==0.23.0\n",
    "!pip install flash-attn --no-build-isolation #--no-cache-dir\n",
    "!pip install --upgrade transformers\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCIaDBL8_ecf"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Config\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-5Erauh_gAv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import orjson\n",
    "import signal\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adafactor\n",
    "# from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    PreTrainedTokenizerFast,\n",
    "    GPT2Config,\n",
    ")\n",
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "class STSDataset(Dataset):\n",
    "  def __init__(self, file_path, tokenizer, max_len=128):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "    self.data = pd.read_csv(file_path, sep='\\t', quoting=3, dtype=str, keep_default_na=False).fillna('') #quoting=3은 따옴표 무시\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    row = self.data.iloc[idx]\n",
    "    sentence1 = str(row.sentence1)\n",
    "    sentence2= str(row.sentence2)\n",
    "    score = float(row.score)\n",
    "\n",
    "    try:\n",
    "        inputs1 = self.tokenizer(sentence1, return_tensors='pt', max_length=self.max_len, padding='max_length', truncation=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[STS ERROR] sentence1 idx={idx}: {repr(sentence1)}\")\n",
    "        print(\"Exception:\", e)\n",
    "        # 건너뛰려면 리턴 None 혹은 임의값, 혹은 exception 재발생\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        inputs2 = self.tokenizer(sentence2, return_tensors='pt', max_length=self.max_len, padding='max_length', truncation=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[STS ERROR] sentence2 idx={idx}: {repr(sentence2)}\")\n",
    "        print(\"Exception:\", e)\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'input_ids1': inputs1['input_ids'].squeeze(0),\n",
    "        'attention_mask1': inputs1['attention_mask'].squeeze(0),\n",
    "        'input_ids2': inputs2['input_ids'].squeeze(0),\n",
    "        'attention_mask2': inputs2['attention_mask'].squeeze(0),\n",
    "        'score': torch.tensor(score, dtype=torch.float)\n",
    "    }\n",
    "\n",
    "class STSModel(nn.Module):\n",
    "  def __init__(self, pretrained_model):\n",
    "    super().__init__()\n",
    "    self.model = pretrained_model\n",
    "    self.config = self.model.config\n",
    "    self.regression_head = nn.Sequential(\n",
    "        nn.Linear(self.config.hidden_size * 3, self.config.hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.config.hidden_size, 1)\n",
    "    )\n",
    "\n",
    " # 토큰 임베딩 평균\n",
    "  def _mean_pooling(self, model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# 문장1, 문장2 임베딩, 차이 벡터 합쳐 *MLP 회귀\n",
    "  def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "    outputs1 = self.model(input_ids=input_ids1, attention_mask=attention_mask1)\n",
    "    outputs2 = self.model(input_ids=input_ids2, attention_mask=attention_mask2)\n",
    "    embedding1 = self._mean_pooling(outputs1, attention_mask1)\n",
    "    embedding2 = self._mean_pooling(outputs2, attention_mask2)\n",
    "    diff = torch.abs(embedding1 - embedding2)\n",
    "    combined_embedding = torch.cat([embedding1, embedding2, diff], dim=1)\n",
    "    return self.regression_head(combined_embedding)\n",
    "\n",
    "\n",
    "# STS 미세튜닝 함수\n",
    "def findtune_and_evaluate_sts(\n",
    "    main_model,\n",
    "    sts_train_dataloader,\n",
    "    sts_dev_dataloader,\n",
    "    device,\n",
    "    finetune_epochs=3,\n",
    "    lr=2e-5\n",
    "):\n",
    "  print(\"STS 파인튜닝 시작\")\n",
    "\n",
    "  base_model = main_model.transformer if hasattr(main_model, 'transformer') else main_model\n",
    "  sts_model = STSModel(base_model).to(device)\n",
    "\n",
    "  optimizer = Adafactor(sts_model.parameters(), lr=lr)\n",
    "  loss_fn = nn.MSELoss()\n",
    "\n",
    "  best_spearman = -1.0\n",
    "\n",
    "  for epoch in range(finetune_epochs):\n",
    "    sts_model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in sts_train_dataloader:\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      input_ids1 = batch['input_ids1'].to(device)\n",
    "      attention_mask1 = batch['attention_mask1'].to(device)\n",
    "      input_ids2 = batch['input_ids2'].to(device)\n",
    "      attention_mask2 = batch['attention_mask2'].to(device)\n",
    "      scores = batch['score'].to(device)\n",
    "\n",
    "      with autocast():\n",
    "        predicted_scores = sts_model(input_ids1, attention_mask1, input_ids2, attention_mask2).squeeze(-1)\n",
    "        loss = loss_fn(predicted_scores, scores)\n",
    "\n",
    "      total_train_loss += loss.item()\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(sts_train_dataloader)\n",
    "    print(f\"STS 파인튜닝 Epoch {epoch+1} - Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # 매 에포크 후 dev 셋으로 검증\n",
    "    sts_model.eval()\n",
    "    real_scores = []\n",
    "    model_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for batch in sts_dev_dataloader:\n",
    "        input_ids1 = batch['input_ids1'].to(device)\n",
    "        attention_mask1 = batch['attention_mask1'].to(device)\n",
    "        input_ids2 = batch['input_ids2'].to(device)\n",
    "        attention_mask2 = batch['attention_mask2'].to(device)\n",
    "\n",
    "        with autocast():\n",
    "          predicted_scores = sts_model(input_ids1, attention_mask1, input_ids2, attention_mask2).squeeze(-1)\n",
    "\n",
    "        model_scores.extend(predicted_scores.cpu().numpy())\n",
    "        real_scores.extend(batch['score'].cpu().numpy())\n",
    "\n",
    "    spearman_corr, _ = spearmanr(real_scores, model_scores)\n",
    "    print(f\"STS 파인튜닝 Epoch {epoch+1} - Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "    if spearman_corr > best_spearman:\n",
    "      best_spearman = spearman_corr\n",
    "\n",
    "  print(f\"STS 파인튜닝 및 검증 종료. 최고 점수: {best_spearman:.4f}\")\n",
    "\n",
    "  return best_spearman\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Dataset 정의\n",
    "# ----------------------------\n",
    "class JsonlTextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        block_size: int = 2048,\n",
    "        stride: int = 256,\n",
    "    ):\n",
    "        self.examples = []\n",
    "\n",
    "        bos_id = tokenizer.bos_token_id\n",
    "        eos_id = tokenizer.eos_token_id\n",
    "        pad_id = tokenizer.pad_token_id\n",
    "\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.jsonl\"))\n",
    "        for path in files:\n",
    "            with open(path, \"rb\") as f:\n",
    "                for line_no, line in enumerate(tqdm(f, desc=f'Loading {os.path.basename(path)}', leave=True)):\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        obj = orjson.loads(line)\n",
    "                    except orjson.JSONDecodeError as e:\n",
    "                        print(f\"[JSONL ERROR] {path} line {line_no}: malformed JSON\")\n",
    "                        print(\"  >>\", line)\n",
    "                        continue\n",
    "\n",
    "                    text = obj.get(\"text\", \"\").strip()\n",
    "                    if not text:\n",
    "                        continue\n",
    "\n",
    "                    # 토크나이저 에러 처리\n",
    "                    try:\n",
    "                        raw_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[TOKENIZER ERROR] {path} line {line_no}: cannot tokenize text\")\n",
    "                        print(\"  >>\", repr(text))\n",
    "                        print(\"Exception:\", e)\n",
    "                        continue\n",
    "\n",
    "                    if len(raw_ids) == 0:\n",
    "                        continue\n",
    "\n",
    "                    raw_ids = [bos_id] + raw_ids + [eos_id]\n",
    "\n",
    "                    # 2) 슬라이딩 윈도우\n",
    "                    step = block_size - stride\n",
    "                    for start in range(0, len(raw_ids), step):\n",
    "                        chunk = raw_ids[start : start + block_size]\n",
    "\n",
    "                        if len(chunk) < block_size:\n",
    "                            chunk = chunk + [pad_id] * (block_size - len(chunk))\n",
    "\n",
    "                        self.examples.append(torch.tensor(chunk, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.examples[idx]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\":    input_ids.clone()  # causal LM\n",
    "        }\n",
    "\n",
    "def find_latest_checkpoint(ckpt_dir: str):\n",
    "    \"\"\"\n",
    "    ckpt_dir/checkpoint-<step>.pt 파일들 중\n",
    "    가장 step 이 큰 파일 경로, 그리고 그 스텝 번호를 리턴합니다.\n",
    "    없으면 (None, None).\n",
    "    \"\"\"\n",
    "    paths = glob.glob(os.path.join(ckpt_dir, \"checkpoint-*.pt\"))\n",
    "    if not paths:\n",
    "        return None, None\n",
    "    # 파일명에서 숫자만 추출 (checkpoint-1234.pt → 1234)\n",
    "    def extract_step(path):\n",
    "        base = os.path.basename(path)\n",
    "        num = base.replace(\"checkpoint-\", \"\").replace(\".pt\", \"\")\n",
    "        return int(num)\n",
    "    steps = [extract_step(p) for p in paths]\n",
    "    idx = int(steps.index(max(steps)))\n",
    "    return paths[idx], steps[idx]\n",
    "\n",
    "def save_checkpoint(output_dir, model, optimizer, scheduler, step):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    tmp_path   = os.path.join(output_dir, f\"checkpoint-{step}.pt.tmp\")\n",
    "    final_path = os.path.join(output_dir, f\"checkpoint-{step}.pt\")\n",
    "\n",
    "    # 1) temp 파일에 저장\n",
    "    with open(tmp_path, \"wb\") as f:\n",
    "        torch.save({\n",
    "            \"step\":      step,\n",
    "            \"model\":     model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "        }, f)\n",
    "        f.flush()\n",
    "        os.fsync(f.fileno())   # 디스크에 완전 기록 보장\n",
    "\n",
    "    # 2) atomic rename\n",
    "    os.replace(tmp_path, final_path)\n",
    "    print(f\"*** Saved checkpoint: {final_path}\")\n",
    "\n",
    "def register_signal_handlers(output_dir, model, optimizer, scheduler, get_step_fn):\n",
    "    def handler(signum, frame):\n",
    "        step = get_step_fn()\n",
    "        print(f\"\\n=== Received signal {signum}, saving final checkpoint ...\")\n",
    "        save_checkpoint(output_dir, model, optimizer, scheduler, step)\n",
    "        exit(0)\n",
    "\n",
    "    signal.signal(signal.SIGINT, handler)   # Ctrl-C\n",
    "    signal.signal(signal.SIGTERM, handler)  # kill\n",
    "\n",
    "def get_model_info(model):\n",
    "   # ============================================================================\n",
    "    # 1. 기본 모델 정보\n",
    "    # ============================================================================\n",
    "    print(\"\\n [모델 기본 정보]\")\n",
    "    print(f\"모델 타입: {model.config.model_type}\")\n",
    "    print(f\"모델 아키텍처: {model.__class__.__name__}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # 2. 모델 설정(config) 확인\n",
    "    # ============================================================================\n",
    "    print(\"\\n [모델 설정 정보]\")\n",
    "    print(f\"히든 레이어 크기 (hidden_size): {model.config.hidden_size}\")\n",
    "    print(f\"디코더 레이어 수 (num_hidden_layers): {model.config.num_hidden_layers}\")\n",
    "    print(f\"어텐션 헤드 수 (num_attention_heads): {model.config.num_attention_heads}\")\n",
    "\n",
    "    # Phi-3는 intermediate_size가 없을 수 있으므로 안전하게 처리\n",
    "    if hasattr(model.config, 'intermediate_size'):\n",
    "        print(f\"중간 레이어 크기 (intermediate_size): {model.config.intermediate_size}\")\n",
    "    else:\n",
    "        # FFN 크기는 보통 hidden_size의 4배\n",
    "        estimated_intermediate = model.config.hidden_size * 4\n",
    "        print(f\"중간 레이어 크기 (추정): {estimated_intermediate}\")\n",
    "\n",
    "    print(f\"어휘 크기 (vocab_size): {model.config.vocab_size}\")\n",
    "    print(f\"최대 시퀀스(문맥) 길이 (max_position_embeddings): {model.config.max_position_embeddings}\")\n",
    "\n",
    "\n",
    "    # 추가 정보 (Phi-3 특화)\n",
    "    if hasattr(model.config, 'num_key_value_heads'):\n",
    "        print(f\"Key-Value 헤드 수 (GQA): {model.config.num_key_value_heads}\")\n",
    "    if hasattr(model.config, 'rope_theta'):\n",
    "        print(f\"RoPE Theta: {model.config.rope_theta}\")\n",
    "    if hasattr(model.config, 'sliding_window'):\n",
    "        print(f\"Sliding Window: {model.config.sliding_window}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # 3. 전체 파라미터 통계\n",
    "    # ============================================================================\n",
    "    print(\"\\n [파라미터 통계]\")\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen_params = total_params - trainable_params\n",
    "\n",
    "    print(f\"전체 파라미터: {total_params:,}개 ({total_params / 1e9:.2f}B)\")\n",
    "    print(f\"학습 가능 파라미터: {trainable_params:,}개 ({trainable_params / 1e9:.2f}B)\")\n",
    "    print(f\"고정 파라미터: {frozen_params:,}개\")\n",
    "\n",
    "    # 메모리 사용량 추정\n",
    "    mem_fp32 = (total_params * 4) / (1024 ** 2)\n",
    "    mem_fp16 = mem_fp32 / 2\n",
    "    print(f\"예상 메모리:\")\n",
    "    print(f\"  • Float32: {mem_fp32:.2f} MB ({mem_fp32/1024:.2f} GB)\")\n",
    "    print(f\"  • Float16: {mem_fp16:.2f} MB ({mem_fp16/1024:.2f} GB)\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # 4. 모델 구조 계층 분석\n",
    "    # ============================================================================\n",
    "    print(\"\\n [모델 구조 계층]\")\n",
    "\n",
    "    # 모델의 주요 컴포넌트 확인\n",
    "    print(\"주요 컴포넌트:\")\n",
    "    for name, module in model.named_children():\n",
    "        module_params = sum(p.numel() for p in module.parameters())\n",
    "        print(f\"  • {name}: {module_params:,}개 파라미터\")\n",
    "\n",
    "\n",
    "    # ============================================================================\n",
    "    # 5. 카테고리별 파라미터 분석\n",
    "    # ============================================================================\n",
    "    print(\"\\n [카테고리별 파라미터 분석]\")\n",
    "\n",
    "    categories = {\n",
    "        'Embeddings  (임베딩)': 0,\n",
    "        'Attention   (어텐션)': 0,\n",
    "        'MLP/FFN (피드포워드)': 0,\n",
    "        'LayerNorm   (정규화)': 0,\n",
    "        'Output Head (출력층)': 0,\n",
    "        'Others        (기타)': 0\n",
    "    }\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "\n",
    "        # 카테고리 분류\n",
    "        if 'embed' in name.lower():\n",
    "            categories['Embeddings  (임베딩)'] += param_count\n",
    "        elif 'attn' in name.lower() or 'attention' in name.lower():\n",
    "            categories['Attention   (어텐션)'] += param_count\n",
    "        elif 'mlp' in name.lower() or 'ffn' in name.lower() or 'fc' in name.lower():\n",
    "            categories['MLP/FFN (피드포워드)'] += param_count\n",
    "        elif 'norm' in name.lower() or 'ln' in name.lower():\n",
    "            categories['LayerNorm   (정규화)'] += param_count\n",
    "        elif 'lm_head' in name.lower() or 'output' in name.lower():\n",
    "            categories['Output Head (출력층)'] += param_count\n",
    "        else:\n",
    "            categories['Others        (기타)'] += param_count\n",
    "\n",
    "    print(f\"{'카테고리':<30} {'파라미터 수':<20} {'비율'}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    for category, count in categories.items():\n",
    "        if count > 0:\n",
    "            percentage = (count / total_params) * 100\n",
    "            print(f\"{category:<20} {count:>20,}개 {percentage:>6.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRKLb31A_h9v"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content/drive/MyDrive/Colab Notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrFoDLFM_jMY",
    "outputId": "0a38c157-e40b-4a5c-fb7a-bdc964e151eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 데이터셋 파일이 존재하지 않아 새로 생성합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading 한국어_데이터셋_3_1.jsonl: 459857it [27:58, 25.34it/s]"
     ]
    }
   ],
   "source": [
    "cached_dataset_path = \"cached_full_dataset.pt\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1) 토크나이저 / 데이터셋\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"coreAI_tokenizer\")\n",
    "block_size = 2048\n",
    "stride = 256\n",
    "if os.path.exists(cached_dataset_path):\n",
    "  print(\"전처리 데이터셋 파일이 존재하여 불러옵니다.\")\n",
    "  full_dataset = torch.load(cached_dataset_path,weights_only=False)\n",
    "else:\n",
    "  print(\"전처리 데이터셋 파일이 존재하지 않아 새로 생성합니다.\")\n",
    "  full_dataset = JsonlTextDataset(\"data\", tokenizer, block_size=block_size, stride=stride)\n",
    "  print(\"전처리 데이터셋 생성 완료\")\n",
    "  torch.save(full_dataset, cached_dataset_path)\n",
    "  print(\"데이터셋 저장 완료\")\n",
    "\n",
    "split_rate = 0.975\n",
    "train_size = int(split_rate*len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_ds, val_ds = random_split(full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "grad_accum_steps = 32\n",
    "per_device_batch_size = 512 // grad_accum_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9oKvtbx_lI3"
   },
   "outputs": [],
   "source": [
    "grad_accum_steps = 64\n",
    "per_device_batch_size = 512 // grad_accum_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQhcNXsk_m3i"
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=per_device_batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=per_device_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b33vze7_otP"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  sts_dev_dataset = STSDataset(\"data/STSData/sts-dev.tsv\", tokenizer)\n",
    "  sts_dev_dataloader = DataLoader(sts_dev_dataset, batch_size=per_device_batch_size, shuffle=False)\n",
    "  sts_train_dataset = STSDataset(\"data/STSData/sts-train.tsv\", tokenizer)\n",
    "  sts_train_dataloader = DataLoader(sts_train_dataset, batch_size=per_device_batch_size, shuffle=True)\n",
    "  run_sts_eval = True\n",
    "  print(\"STS 데이터셋 로드 완료\")\n",
    "except FileNotFoundError:\n",
    "  print(\"Warning: STS 데이터셋이 없습니다. STS 평가는 건너뜁니다.\")\n",
    "  run_sts_eval = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Py-s7Z5m_qrB"
   },
   "outputs": [],
   "source": [
    "bos_id = tokenizer.bos_token_id\n",
    "eos_id = tokenizer.eos_token_id\n",
    "pad_id = tokenizer.pad_token_id\n",
    "unk_id = tokenizer.unk_token_id\n",
    "\n",
    "# 2) 모델 & optimizer & scheduler & scaler\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12,\n",
    "    n_positions=block_size,\n",
    "    bos_token_id=bos_id,\n",
    "    eos_token_id=eos_id,\n",
    "    pad_token_id=pad_id,\n",
    "    unk_token_id=unk_id,\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attn_pdrop=0.1,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_config(config,attn_implementation=\"flash_attention_2\",).to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "optimizer = Adafactor(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "total_steps = 50000\n",
    "max_grad_norm = 1.0\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=total_steps, eta_min=1e-6\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 3) 체크포인트 복원 시도\n",
    "ckpt_dir = \"checkpoints\"\n",
    "latest_ckpt, last_step = find_latest_checkpoint(ckpt_dir)\n",
    "if latest_ckpt:\n",
    "    print(f\"Loading checkpoint {latest_ckpt} (step={last_step}) …\")\n",
    "    ckpt = torch.load(latest_ckpt, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
    "    global_step = ckpt[\"step\"]\n",
    "else:\n",
    "    print(\"No checkpoint found, starting from scratch.\")\n",
    "    global_step = 0\n",
    "\n",
    "wandb.login(key=\"d5b25fa78b19fef961f3f6b203f821bd5d2c5b91\")\n",
    "wandb.init(\n",
    "    project=\"CoreAI_pretrained\",           # W&B 웹에서 만들 프로젝트 이름\n",
    "    name=f\"run-{os.getpid()}\",            # (옵션) 실험 이름\n",
    "    config={                              # hyperparameter 로깅\n",
    "        \"lr\":           3e-4,\n",
    "        \"batch_size\":   per_device_batch_size,\n",
    "        \"grad_accum\":   grad_accum_steps,\n",
    "        \"total_steps\":  total_steps,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"model_config\": config.to_dict(), # 모델 configuration 도 함께\n",
    "    }\n",
    ")\n",
    "\n",
    "print(get_model_info(model))\n",
    "\n",
    "# 모델 파라미터/그래디언트 추적(logging) (옵션)\n",
    "wandb.watch(model, log=\"all\", log_freq=100)\n",
    "\n",
    "# 4) 학습 루프\n",
    "validation_interval = 200\n",
    "model.train()\n",
    "register_signal_handlers(ckpt_dir, model, optimizer, scheduler, get_step_fn=lambda: global_step)\n",
    "\n",
    "for epoch in range(1):\n",
    "    pbar = tqdm(train_dl, desc=f\"Epoch {epoch}\", leave=True)\n",
    "    for step, batch in enumerate(pbar):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels    = batch[\"labels\"].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss / grad_accum_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % grad_accum_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % validation_interval == 0:\n",
    "                model.eval()\n",
    "                total_val_loss = 0.0\n",
    "                num_val_batches = 0\n",
    "                with torch.no_grad():\n",
    "                    print(\"검증 중\")\n",
    "                    for val_batch in val_dl:\n",
    "                        val_input_ids = val_batch[\"input_ids\"].to(device)\n",
    "                        val_labels    = val_batch[\"labels\"].to(device)\n",
    "\n",
    "                        with autocast():\n",
    "                            val_outputs = model(input_ids=val_input_ids, labels=val_labels)\n",
    "                            total_val_loss += val_outputs.loss.item()\n",
    "                            num_val_batches += 1\n",
    "\n",
    "                avg_val_loss = total_val_loss / num_val_batches\n",
    "                perplexity = torch.exp(torch.tensor(avg_val_loss)).item()\n",
    "                print(f\"*** Validation Step {global_step} - Loss: {avg_val_loss:.4f}, Perplexity: {perplexity:.2f}\")\n",
    "                wandb.log({\n",
    "                    \"eval/loss\":      avg_val_loss,\n",
    "                    \"eval/perplexity\": perplexity,\n",
    "                }, step=global_step)\n",
    "\n",
    "                if run_sts_eval:\n",
    "                  spearman_correlation = findtune_and_evaluate_sts(model, sts_train_dataloader, sts_dev_dataloader, device)\n",
    "                  wandb.log({\n",
    "                      \"sts/spearman\": spearman_correlation\n",
    "                  }, step=global_step)\n",
    "\n",
    "                model.train()\n",
    "\n",
    "\n",
    "            cur_loss = loss.item() * grad_accum_steps\n",
    "            cur_lr   = scheduler.get_last_lr()[0]\n",
    "\n",
    "            # tqdm\n",
    "            pbar.set_postfix({\n",
    "                \"step\": global_step,\n",
    "                \"loss\": f\"{cur_loss:.4f}\",\n",
    "                \"lr\":   f\"{cur_lr:.2e}\"\n",
    "            })\n",
    "\n",
    "            # --------------------------------------------\n",
    "            # (★) W&B 에 로그 기록\n",
    "            # --------------------------------------------\n",
    "            wandb.log({\n",
    "                \"train/loss\": cur_loss,\n",
    "                \"train/lr\":   cur_lr,\n",
    "                \"step\":       global_step\n",
    "            }, step=global_step)\n",
    "\n",
    "            # if global_step % 100 == 0:\n",
    "            #     # loss 에 grad_accum_steps 곱해서 원래 스케일로 복원\n",
    "            #     print(f\"Epoch {epoch} Step {global_step} Loss {(loss.item() * grad_accum_steps):.4f}\")\n",
    "\n",
    "            # 매 100 스텝마다 체크포인트 저장\n",
    "            if global_step > 0 and global_step % 100 == 0:\n",
    "                save_checkpoint(ckpt_dir, model, optimizer, scheduler, global_step)\n",
    "\n",
    "            if global_step >= total_steps:\n",
    "                break\n",
    "\n",
    "    if global_step >= total_steps:\n",
    "        break\n",
    "\n",
    "if global_step > 0:\n",
    "    save_checkpoint(ckpt_dir, model, optimizer, scheduler, global_step)\n",
    "\n",
    "# 2-4) 모델&토크나이저 저장\n",
    "model_dir = \"foundation_ckpt\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "# 아티팩트 생성 및 로깅\n",
    "artifact = wandb.Artifact(\n",
    "    name=\"gpt2-pretrained\",   # 아티팩트 이름\n",
    "    type=\"model\",             # 유형(모델, 데이터셋 등)\n",
    "    metadata={\"step\": global_step}\n",
    ")\n",
    "artifact.add_dir(model_dir)\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "# 실험 종료 알림\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
